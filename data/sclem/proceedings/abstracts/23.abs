Most of neural language models use different kinds of embeddings for
	  word prediction. While word embeddings can be associated to each
	  word in the vocabulary or derived from characters as well as
	  factored morphological decomposition, these word representations are
	  mainly used to parametrize the input, i.e. the context of
	  prediction.  This work investigates the effect of using subword
	  units (character and factored morphological decomposition) to build
	  output representations for neural language modeling. We present a
	  case study on Czech, a morphologically-rich language, experimenting
	  with different input and output representations.  When working with
	  the full training vocabulary, despite unstable training, our
	  experiments show that augmenting the output word representations
	  with character-based embeddings can significantly improve the
	  performance of the model. Moreover, reducing the size of the output
	  look-up table, to let the character-based embeddings represent rare
	  words, brings further improvement.