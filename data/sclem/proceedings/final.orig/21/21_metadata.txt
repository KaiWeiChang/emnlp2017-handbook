SubmissionNumber#=%=#21
FinalPaperTitle#=%=#(EXTENDED ABSTRACT) Natural Language Generation through Character-Based RNNs with Finite-State Prior Knowledge
ShortPaperTitle#=%=#
NumberOfPages#=%=#
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#Recently \newcite{wen2015semantically} have proposed a Recurrent Neural Network
(RNN) approach to the generation of utterances from dialog acts, and shown that
although their model requires less effort to develop than a rule-based system,
it is able to improve certain aspects of the utterances, in particular their
naturalness. However their system employs generation at the word-level, which
requires one to pre-process the data by substituting named entities with
placeholders. This pre-processing prevents the model from handling some
contextual effects and from managing multiple occurrences of the same
attribute.

Our approach uses a character-level model, which unlike the word-level model
makes it possible to learn to ``copy" information from the dialog act to the
target without having to pre-process the input. In order to avoid generating
non-words and inventing information not present in the input, we propose a
method for incorporating prior knowledge into the RNN in the form of a weighted
finite-state automaton over character sequences. Automatic and human
evaluations show improved performance over baselines on several evaluation
criteria.

\emph{To SCLeM reviewers: this ``Extended Abstract'' submission was previously
published in Coling-2016.}
Author{1}{Firstname}#=%=#Raghav
Author{1}{Lastname}#=%=#Goyal
Author{1}{Email}#=%=#raghavgoyal14@gmail.com
Author{1}{Affiliation}#=%=#Xerox Research Centre Europe
Author{2}{Firstname}#=%=#Marc
Author{2}{Lastname}#=%=#Dymetman
Author{2}{Email}#=%=#marc.dymetman@xrce.xerox.com
Author{2}{Affiliation}#=%=#Xerox Research Centre Europe
Author{3}{Firstname}#=%=#Eric
Author{3}{Lastname}#=%=#Gaussier
Author{3}{Email}#=%=#eric.gaussier@imag.fr
Author{3}{Affiliation}#=%=#Univ. Grenoble 1

==========