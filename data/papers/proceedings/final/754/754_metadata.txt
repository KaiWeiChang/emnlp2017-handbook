SubmissionNumber#=%=#754
FinalPaperTitle#=%=#Unsupervised Pretraining for Sequence to Sequence Learning
ShortPaperTitle#=%=#Unsupervised Pretraining for Sequence to Sequence Learning
NumberOfPages#=%=#9
CopyrightSigned#=%=#Prajit Ramachandran
JobTitle#==#
Organization#==#Google
1600 Amphitheatre Pkwy, Mountain View, CA
Abstract#==#This work presents a general unsupervised learning method to improve the
accuracy of sequence to sequence (seq2seq) models. In our method, the weights
of the encoder and decoder of a seq2seq model are initialized with the
pretrained weights of two language models and then fine-tuned with labeled
data. We apply this method to challenging benchmarks in machine translation and
abstractive summarization and find that it significantly improves the
subsequent supervised models.  Our main result is that pretraining improves the
generalization of seq2seq models. We achieve state-of-the-art results on the
WMT English$\rightarrow$German task, surpassing a range of methods using both
phrase-based machine translation and neural machine translation. Our method
achieves a significant improvement of 1.3 BLEU from th previous best models on
both WMT'14 and WMT'15 English$\rightarrow$German. We also conduct human
evaluations on abstractive summarization and find that our method outperforms a
purely supervised learning baseline in a statistically significant manner.
Author{1}{Firstname}#=%=#Prajit
Author{1}{Lastname}#=%=#Ramachandran
Author{1}{Email}#=%=#prmchnd2@illinois.edu
Author{1}{Affiliation}#=%=#University of Illinois at Urbana-Champaign
Author{2}{Firstname}#=%=#Peter
Author{2}{Lastname}#=%=#Liu
Author{2}{Email}#=%=#peterjliu@google.com
Author{2}{Affiliation}#=%=#Google Brain
Author{3}{Firstname}#=%=#Quoc
Author{3}{Lastname}#=%=#Le
Author{3}{Email}#=%=#qvl@google.com
Author{3}{Affiliation}#=%=#Google Brain

==========