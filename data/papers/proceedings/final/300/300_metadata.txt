SubmissionNumber#=%=#300
FinalPaperTitle#=%=#Analogs of Linguistic Structure in Deep Representations
ShortPaperTitle#=%=#Analogs of Linguistic Structure in Deep Representations
NumberOfPages#=%=#5
CopyrightSigned#=%=#Jacob Andreas
JobTitle#==#
Organization#==#
Abstract#==#We investigate the compositional structure of message vectors computed by a
deep
network trained on a communication game. By comparing truth-conditional
representations of encoder-produced message vectors to human-produced referring
expressions, we are able to identify aligned (vector, utterance) pairs with the
same meaning. We then search for structured relationships among these aligned
pairs to discover simple vector space transformations corresponding to
negation,
conjunction, and disjunction. Our results suggest that neural representations
are capable of spontaneously developing a ``syntax'' with functional analogues
to qualitative properties of natural language.
Author{1}{Firstname}#=%=#Jacob
Author{1}{Lastname}#=%=#Andreas
Author{1}{Email}#=%=#jda@cs.berkeley.edu
Author{1}{Affiliation}#=%=#Berkeley
Author{2}{Firstname}#=%=#Dan
Author{2}{Lastname}#=%=#Klein
Author{2}{Email}#=%=#klein@cs.berkeley.edu
Author{2}{Affiliation}#=%=#UC Berkeley

==========