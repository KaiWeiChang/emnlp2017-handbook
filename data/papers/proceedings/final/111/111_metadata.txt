SubmissionNumber#=%=#111
FinalPaperTitle#=%=#Learning What's Easy: Fully Differentiable Neural Easy-First Taggers
ShortPaperTitle#=%=#Learning What's Easy: Fully Differentiable Neural Easy-First Taggers
NumberOfPages#=%=#12
CopyrightSigned#=%=#Andre Martins
JobTitle#==#
Organization#==#Unbabel, Lisbon, Portugal
Abstract#==#We introduce a novel neural easy-first decoder that learns to solve sequence
tagging tasks in a flexible order. In contrast to previous easy-first decoders,
our models are end-to-end differentiable. The decoder iteratively updates a
“sketch” of the predictions over the sequence. At its core is an attention
mechanism that controls which parts of the input are strategically the best to
process next. We present a new constrained softmax transformation that ensures
the same cumulative attention to every word, and show how to efficiently
evaluate and backpropagate over it. Our models compare favourably to BILSTM
taggers on three sequence tagging tasks.
Author{1}{Firstname}#=%=#André F. T.
Author{1}{Lastname}#=%=#Martins
Author{1}{Email}#=%=#afm@cs.cmu.edu
Author{1}{Affiliation}#=%=#Priberam, Instituto de Telecomunicacoes
Author{2}{Firstname}#=%=#Julia
Author{2}{Lastname}#=%=#Kreutzer
Author{2}{Email}#=%=#kreutzer@cl.uni-heidelberg.de
Author{2}{Affiliation}#=%=#Department of Computational Linguistics, Heidelberg University

==========