SubmissionNumber#=%=#1430
FinalPaperTitle#=%=#Sequence Effects in Crowdsourced Annotations
ShortPaperTitle#=%=#Sequence Effects in Crowdsourced Annotations
NumberOfPages#=%=#6
CopyrightSigned#=%=#Nitika Mathur
JobTitle#==#
Organization#==#The University of Melbourne
CIS, The University of Melbourne VIC 3010 Australia
Abstract#==#Manual data annotation is a vital component of NLP research. When designing
annotation tasks, properties of the annotation interface can unintentionally
lead to artefacts in the resulting dataset, biasing the evaluation. In this
paper, we explore sequence effects where annotations of an item are affected by
the preceding items. Having assigned one label to an instance, the annotator
may be less (or more) likely to assign the same label to the next. During
rating tasks, seeing a low quality item may affect the score given to the next
item either positively or negatively. We see clear evidence of both types of
effects using auto-correlation studies over three different crowdsourced
datasets. We then recommend a simple way to minimise sequence effects.
Author{1}{Firstname}#=%=#Nitika
Author{1}{Lastname}#=%=#Mathur
Author{1}{Email}#=%=#nmathur@student.unimelb.edu.au
Author{1}{Affiliation}#=%=#The University of Melbourne
Author{2}{Firstname}#=%=#Timothy
Author{2}{Lastname}#=%=#Baldwin
Author{2}{Email}#=%=#tb@ldwin.net
Author{2}{Affiliation}#=%=#The University of Melbourne
Author{3}{Firstname}#=%=#Trevor
Author{3}{Lastname}#=%=#Cohn
Author{3}{Email}#=%=#tcohn@unimelb.edu.au
Author{3}{Affiliation}#=%=#University of Melbourne

==========