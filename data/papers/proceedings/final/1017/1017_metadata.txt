SubmissionNumber#=%=#1017
FinalPaperTitle#=%=#No Need to Pay Attention: Simple Recurrent Neural Networks Work!
ShortPaperTitle#=%=#No Need to Pay Attention: Simple Recurrent Neural Networks Work!
NumberOfPages#=%=#7
CopyrightSigned#=%=#Ferhan Ture
JobTitle#==#
Organization#==#
Abstract#==#First-order factoid question answering assumes that the question can be
answered by a single fact in a knowledge base (KB). While this does not seem
like a challenging task, many recent attempts that apply either complex
linguistic reasoning or deep neural networks achieve 65\%--76\% accuracy on
benchmark
sets. Our approach formulates the task as two machine learning problems:\
detecting the entities in the question, and classifying the question as one of
the relation types in the KB. We train a recurrent neural network to solve each
problem. On the SimpleQuestions dataset, our approach yields substantial
improvements over previously published results --- even neural networks based
on much more complex architectures. The simplicity of our approach also has
practical advantages, such as efficiency and modularity, that are valuable
especially in an industry setting. In fact, we present a preliminary analysis
of the performance of our model on real queries from Comcast's X1 entertainment
platform with millions of users every day.
Author{1}{Firstname}#=%=#Ferhan
Author{1}{Lastname}#=%=#Ture
Author{1}{Email}#=%=#ferhan.ture@gmail.com
Author{1}{Affiliation}#=%=#Comcast Labs
Author{2}{Firstname}#=%=#Oliver
Author{2}{Lastname}#=%=#Jojic
Author{2}{Email}#=%=#oliver_jojic@cable.comcast.com
Author{2}{Affiliation}#=%=#Comcast Labs

==========