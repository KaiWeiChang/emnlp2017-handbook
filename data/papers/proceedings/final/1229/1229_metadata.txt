SubmissionNumber#=%=#1229
FinalPaperTitle#=%=#Why ADAGRAD Fails for Online Topic Modeling
ShortPaperTitle#=%=#Why ADAGRAD Fails for Online Topic Modeling
NumberOfPages#=%=#6
CopyrightSigned#=%=#You Lu
JobTitle#==#Student
Organization#==#Computer Science Department, University of Colorado Boulder
Abstract#==#Online topic modeling, i.e., topic modeling
with stochastic variational inference, is a
powerful and efficient technique for analyzing
large datasets, and ADAGRAD is a
widely-used technique for tuning learning
rates during online gradient optimization.
However, these two techniques do not work
well together. We show that this is because
ADAGRAD uses accumulation of previous
gradients as the learning ratesâ€™ denominators.
For online topic modeling, the magnitude
of gradients is very large. It causes
learning rates to shrink very quickly, so the
parameters cannot fully converge until the
training ends
Author{1}{Firstname}#=%=#You
Author{1}{Lastname}#=%=#Lu
Author{1}{Email}#=%=#you.lu@colorado.edu
Author{1}{Affiliation}#=%=#University of Colorado, Boulder
Author{2}{Firstname}#=%=#Jeffrey
Author{2}{Lastname}#=%=#Lund
Author{2}{Email}#=%=#jefflund@gmail.com
Author{2}{Affiliation}#=%=#Brigham Young University
Author{3}{Firstname}#=%=#Jordan
Author{3}{Lastname}#=%=#Boyd-Graber
Author{3}{Email}#=%=#jbg@umiacs.umd.edu
Author{3}{Affiliation}#=%=#University of Maryland

==========