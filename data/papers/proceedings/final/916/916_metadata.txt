SubmissionNumber#=%=#916
FinalPaperTitle#=%=#Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks
ShortPaperTitle#=%=#Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks
NumberOfPages#=%=#10
CopyrightSigned#=%=#HA
JobTitle#==#
Organization#==#Harvard University
Abstract#==#We present a novel approach for training artificial neural networks. Our
approach is inspired by broad evidence in psychology that shows human learners
can learn efficiently and effectively by increasing intervals of time between
subsequent reviews of previously learned materials (spaced repetition). We
investigate the analogy between training neural models and findings in
psychology about human memory model and develop an efficient and effective
algorithm to train neural models. The core part of our algorithm is a
cognitively-motivated scheduler according to which training instances and their
"reviews" are spaced over time. Our algorithm uses only 34-50% of data per
epoch, is 2.9-4.8 times faster than standard training, and outperforms
competing state-of-the-art baselines. Our code is available at
scholar.harvard.edu/hadi/RbF/.
Author{1}{Firstname}#=%=#Hadi
Author{1}{Lastname}#=%=#Amiri
Author{1}{Email}#=%=#amirieb@gmail.com
Author{1}{Affiliation}#=%=#Harvard University
Author{2}{Firstname}#=%=#Timothy
Author{2}{Lastname}#=%=#Miller
Author{2}{Email}#=%=#timothy.miller@childrens.harvard.edu
Author{2}{Affiliation}#=%=#Boston Children's Hospital and Harvard Medical School
Author{3}{Firstname}#=%=#Guergana
Author{3}{Lastname}#=%=#Savova
Author{3}{Email}#=%=#guergana.savova@childrens.harvard.edu
Author{3}{Affiliation}#=%=#Harvard

==========