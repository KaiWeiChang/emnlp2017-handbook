@InProceedings{han-jiang-tu:0:TOBEFILLED-EduApp,
  author    = {Han, Wenjuan  and  Jiang, Yong  and  Tu, Kewei},
  title     = {Dependency Grammar Induction with Neural Lexicalization and Big Training Data},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {1682--1687},
  abstract  = {We study the impact of big models (in terms of the degree of lexicalization)
	and big data (in terms of the training corpus size) on dependency grammar
	induction.
	We experimented with L-DMV, a lexicalized version of Dependency Model with
	Valence \cite{Klein:2004:CIS:1218955.1219016} and L-NDMV, our lexicalized
	extension of the Neural Dependency Model with Valence
	\cite{jiang-han-tu:2016:EMNLP2016}. 
	We find that L-DMV only benefits from very small degrees of lexicalization and
	moderate sizes of training corpora. L-NDMV can benefit from big training data
	and lexicalization of greater degrees, especially when enhanced with good model
	initialization, and it achieves a result that is competitive with the current
	state-of-the-art.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1176, http://www.aclweb.org/anthology/W17-20 0}
}

