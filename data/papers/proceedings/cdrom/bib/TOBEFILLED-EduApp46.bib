@InProceedings{hashimoto-EtAl:0:TOBEFILLED-EduApp,
  author    = {Hashimoto, Kazuma  and  xiong, caiming  and  Tsuruoka, Yoshimasa  and  Socher, Richard},
  title     = {A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {444--454},
  abstract  = {Transfer and multi-task learning have traditionally focused on either a single
	source-target pair or very few, similar tasks.
	Ideally, the linguistic levels of morphology, syntax and semantics would
	benefit each other by being trained in a single model.
	We introduce a joint many-task model together with a strategy for successively
	growing its depth to solve increasingly complex tasks.
	Higher layers include shortcut connections to lower-level task predictions to
	reflect linguistic hierarchies.
	We use a simple regularization term to allow for optimizing all model weights
	to improve one task's loss without exhibiting catastrophic interference of the
	other tasks.
	Our single end-to-end model obtains state-of-the-art or competitive results on
	five different tasks from tagging, parsing, relatedness, and entailment tasks.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1 46, http://www.aclweb.org/anthology/W17-20 0}
}

