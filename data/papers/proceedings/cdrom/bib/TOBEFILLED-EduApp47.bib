@InProceedings{lu-lund-boydgraber:0:TOBEFILLED-EduApp,
  author    = {Lu, You  and  Lund, Jeffrey  and  Boyd-Graber, Jordan},
  title     = {Why ADAGRAD Fails for Online Topic Modeling},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {455--460},
  abstract  = {Online topic modeling, i.e., topic modeling
	with stochastic variational inference, is a
	powerful and efficient technique for analyzing
	large datasets, and ADAGRAD is a
	widely-used technique for tuning learning
	rates during online gradient optimization.
	However, these two techniques do not work
	well together. We show that this is because
	ADAGRAD uses accumulation of previous
	gradients as the learning ratesâ€™ denominators.
	For online topic modeling, the magnitude
	of gradients is very large. It causes
	learning rates to shrink very quickly, so the
	parameters cannot fully converge until the
	training ends},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1 47, http://www.aclweb.org/anthology/W17-20 0}
}

