@InProceedings{pinter-guthrie-eisenstein:0:TOBEFILLED-EduApp,
  author    = {Pinter, Yuval  and  Guthrie, Robert  and  Eisenstein, Jacob},
  title     = {Mimicking Word Embeddings using Subword RNNs},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {102--112},
  abstract  = {Word embeddings improve generalization over lexical features by placing each
	word in a lower-dimensional space, using distributional information obtained
	from unlabeled data. However, the effectiveness of word embeddings for
	downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which
	embeddings do not exist. In this paper, we present MIMICK, an approach to
	generating OOV word embeddings compositionally, by learning a function from
	spellings to distributional embeddings. Unlike prior work, MIMICK does not
	require re-training on the original word embedding corpus; instead, learning is
	performed at the type level. Intrinsic and extrinsic evaluations demonstrate
	the power of this simple approach. On 23 languages, MIMICK improves
	performance over a word-based baseline for tagging part-of-speech and
	morphosyntactic attributes. It is competitive with (and complementary to) a
	supervised character-based model in low resource settings.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1 10, http://www.aclweb.org/anthology/W17-20 0}
}

