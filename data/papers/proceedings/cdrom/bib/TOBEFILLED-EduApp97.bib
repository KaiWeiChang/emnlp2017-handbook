@InProceedings{vijayakumar-vedantam-parikh:0:TOBEFILLED-EduApp,
  author    = {Vijayakumar, Ashwin  and  Vedantam, Ramakrishna  and  Parikh, Devi},
  title     = {Sound-Word2Vec: Learning Word Representations Grounded in Sounds},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {929--934},
  abstract  = {To be able to interact better with humans, it is crucial for machines to
	understand sound -- a primary modality of human perception. Previous works
	have used sound to learn embeddings for improved generic semantic similarity
	assessment. In this work, we treat sound as a first-class citizen, studying
	downstream 6textual tasks which require aural grounding. To this end, we
	propose sound-word2vec -- a new embedding scheme that learns specialized word
	embeddings grounded in sounds. For example, we learn that two seemingly (se-
	mantically) unrelated concepts, like leaves and paper are similar due to the
	similar rustling sounds they make. Our embed- dings prove useful in textual
	tasks requiring aural reasoning like text-based sound retrieval and discovering
	Foley sound effects (used in movies). Moreover, our em- bedding space captures
	interesting dependencies between words and onomatopoeia and outperforms prior
	work on aurally- relevant word relatedness datasets such as AMEN and ASLex.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1 97, http://www.aclweb.org/anthology/W17-20 0}
}

