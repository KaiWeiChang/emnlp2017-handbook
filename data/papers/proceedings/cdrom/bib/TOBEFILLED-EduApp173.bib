@InProceedings{zheng:0:TOBEFILLED-EduApp,
  author    = {Zheng, Xiaoqing},
  title     = {Incremental Graph-based Neural Dependency Parsing},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {1654--1664},
  abstract  = {Very recently, some studies on neural dependency parsers have shown advantage
	over the traditional ones on a wide variety of languages. However, for
	graph-based neural dependency parsing systems, they either count on the
	long-term memory and attention mechanism to implicitly capture the high-order
	features or give up the global exhaustive inference algorithms in order to
	harness the features over a rich history of parsing decisions. The former might
	miss out the important features for specific headword predictions without the
	help of the explicit structural information, and the latter may suffer from the
	error propagation as false early structural constraints are used to create
	features when making future predictions. We explore the feasibility of
	explicitly taking high-order features into account while remaining the main
	advantage of global inference and learning for graph-based parsing. The
	proposed parser first forms an initial parse tree by head-modifier predictions
	based on the first-order factorization. High-order features (such as
	grandparent, sibling, and uncle) then can be defined over the initial tree, and
	used to refine the parse tree in an iterative fashion. Experimental results
	showed that our model (called INDP) archived competitive performance to
	existing benchmark parsers on both English and Chinese datasets.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1173, http://www.aclweb.org/anthology/W17-20 0}
}

