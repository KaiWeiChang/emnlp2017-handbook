@InProceedings{liu-lapata:0:TOBEFILLED-EduApp,
  author    = {Liu, Yang  and  Lapata, Mirella},
  title     = {Learning Contextually Informed Representations for Linear-Time Discourse Parsing},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {1298--1307},
  abstract  = {Recent advances in RST discourse parsing have focused on two modeling
	paradigms: (a) high order parsers which jointly predict the tree structure of
	the discourse and the relations it encodes; or                                (b)
	linear-time
	parsers
	which
	are efficient but mostly based on local features.  In this work, we propose a
	linear-time parser with a novel way of representing discourse constituents
	based on neural networks which takes into account global contextual information
	and is able to capture long-distance dependencies. Experimental results show
	that our parser obtains state-of-the art performance on benchmark datasets,
	while being efficient (with time complexity linear in the number of sentences
	in the document) and requiring minimal feature engineering.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1134, http://www.aclweb.org/anthology/W17-20 0}
}

