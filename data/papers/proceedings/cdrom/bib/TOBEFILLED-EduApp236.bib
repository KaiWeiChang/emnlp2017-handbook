@InProceedings{novikova-EtAl:0:TOBEFILLED-EduApp,
  author    = {Novikova, Jekaterina  and  Du\v{s}ek, Ond\v{r}ej  and  Cercas Curry, Amanda  and  Rieser, Verena},
  title     = {Why We Need New Evaluation Metrics for NLG},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {2224--2235},
  abstract  = {The majority of NLG evaluation relies on automatic metrics, such as BLEU . In
	this paper, we motivate the need for novel, system- and data-independent
	automatic evaluation methods: We investigate a wide range of metrics, including
	state-of-the-art word-based and novel grammar-based ones, and demonstrate that
	they only weakly reflect human judgements of system outputs as generated by
	data-driven, end-to-end NLG. We also show that metric performance is data- and
	system-specific. Nevertheless, our results also suggest that automatic metrics
	perform reliably at system-level and can support system development by finding
	cases where a system performs poorly.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1236, http://www.aclweb.org/anthology/W17-20 0}
}

