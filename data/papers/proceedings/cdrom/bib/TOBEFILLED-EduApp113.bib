@InProceedings{elsner-shain:0:TOBEFILLED-EduApp,
  author    = {Elsner, Micha  and  Shain, Cory},
  title     = {Speech segmentation with a neural encoder model of working memory},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {1079--1089},
  abstract  = {We present the first unsupervised LSTM speech segmenter as a cognitive model of
	the acquisition of words from unsegmented input. Cognitive biases toward
	phonological and syntactic predictability in speech are rooted in the
	limitations of human memory (Baddeley et al., 1998); compressed representations
	are easier to acquire and retain in memory. To model the biases introduced by
	these memory limitations, our system uses an LSTM-based encoder-decoder with a
	small number of hidden units, then searches for a segmentation that minimizes
	autoencoding loss. Linguistically meaningful segments (e.g. words) should share
	regular patterns of features that facilitate decoder performance in comparison
	to random segmentations, and we show that our learner discovers these patterns
	when trained on either phoneme sequences or raw acoustics. To our knowledge,
	ours is the first fully unsupervised system to be able to segment both symbolic
	and acoustic representations of speech.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1113, http://www.aclweb.org/anthology/W17-20 0}
}

