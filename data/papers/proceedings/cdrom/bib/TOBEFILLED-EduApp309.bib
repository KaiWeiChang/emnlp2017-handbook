@InProceedings{andreas-klein:0:TOBEFILLED-EduApp,
  author    = {Andreas, Jacob  and  Klein, Dan},
  title     = {Analogs of Linguistic Structure in Deep Representations},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {2876--2880},
  abstract  = {We investigate the compositional structure of message vectors computed by a
	deep
	network trained on a communication game. By comparing truth-conditional
	representations of encoder-produced message vectors to human-produced referring
	expressions, we are able to identify aligned (vector, utterance) pairs with the
	same meaning. We then search for structured relationships among these aligned
	pairs to discover simple vector space transformations corresponding to
	negation,
	conjunction, and disjunction. Our results suggest that neural representations
	are capable of spontaneously developing a ``syntax'' with functional analogues
	to qualitative properties of natural language.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1309, http://www.aclweb.org/anthology/W17-20 0}
}

