@InProceedings{sorokin-gurevych:0:TOBEFILLED-EduApp,
  author    = {Sorokin, Daniil  and  Gurevych, Iryna},
  title     = {Context-Aware Representations for Knowledge Base Relation Extraction},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {1784--1789},
  abstract  = {We demonstrate that for sentence-level relation extraction it is beneficial to
	consider other relations in the sentential context while predicting the target
	relation. Our architecture uses an LSTM-based encoder to jointly learn
	representations for all relations in a single sentence.  We combine the context
	representations with an attention mechanism to make the final prediction. 
	We use the Wikidata knowledge base to construct a dataset of multiple relations
	per sentence and to evaluate our approach. Compared to a baseline system, our
	method results in an average error reduction of 24 on a held-out set of
	relations.
	The code and the dataset to replicate the experiments are made available at
	https://github.com/ukplab/.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1188, http://www.aclweb.org/anthology/W17-20 0}
}

