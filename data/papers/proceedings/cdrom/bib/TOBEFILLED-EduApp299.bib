@InProceedings{wang-EtAl:0:TOBEFILLED-EduApp9,
  author    = {Wang, Longyue  and  Tu, Zhaopeng  and  Way, Andy  and  Liu, Qun},
  title     = {Exploiting Cross-Sentence Context for Neural Machine Translation},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {2809--2814},
  abstract  = {In translation, considering the document as a whole can help to resolve
	ambiguities and inconsistencies. In this paper, we propose a cross-sentence
	context-aware approach and investigate the influence of historical contextual
	information on the performance of neural machine translation (NMT). First, this
	history is summarized in a hierarchical way. We then integrate the historical
	representation into NMT in two strategies: 1) a warm-start of encoder and
	decoder states, and 2) an auxiliary context source for updating decoder states.
	Experimental results on a large Chinese-English translation task show that our
	approach significantly improves upon a strong attention-based NMT system by up
	to +2.1 BLEU points.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1299, http://www.aclweb.org/anthology/W17-20 0}
}

