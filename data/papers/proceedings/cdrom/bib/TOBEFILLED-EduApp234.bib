@InProceedings{eshghi-shalyminov-lemon:0:TOBEFILLED-EduApp,
  author    = {Eshghi, Arash  and  Shalyminov, Igor  and  Lemon, Oliver},
  title     = {Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {2203--2213},
  abstract  = {We investigate an end-to-end method for automatically inducing task-based
	dialogue systems from small amounts  of unannotated dialogue data. It combines
	an incremental semantic grammar  - Dynamic Syntax and Type Theory with Records
	(DS-TTR) - with Reinforcement Learning (RL), where language generation and
	dialogue management are a joint  decision problem. The systems thus produced
	are incremental: dialogues are processed word-by-word, shown previously to be
	essential in supporting natural, spontaneous dialogue. We hypothesised that the
	rich linguistic knowledge within the grammar should enable a combinatorially
	large number of dialogue variations to be processed, even when trained on very
	few dialogues. Our experiments show that our model can process 74% of the
	Facebook AI bAbI dataset even when trained on only 0.13% of the data (5
	dialogues).  It can in addition process 65% of bAbI+, a corpus we created by
	systematically adding incremental dialogue phenomena such as restarts and
	self-corrections to bAbI. We compare our model with a state-of-the-art
	retrieval model, MEMN2N. We find that, in terms of semantic accuracy, the
	MEMN2N model shows very poor robustness to the bAbI+ transformations even when
	trained on the full bAbI dataset.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1234, http://www.aclweb.org/anthology/W17-20 0}
}

