@InProceedings{kaji-kobayashi:0:TOBEFILLED-EduApp,
  author    = {Kaji, Nobuhiro  and  Kobayashi, Hayato},
  title     = {Incremental Skip-gram Model with Negative Sampling},
  booktitle = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month     = {TOBEFILLED-June},
  year      = {TOBEFILLED-1},
  address   = {TOBEFILLED-Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {361--369},
  abstract  = {This paper explores an incremental training strategy for the skip-gram model
	with negative sampling (SGNS) from both empirical and theoretical perspectives.
	Existing methods of neural word embeddings, including SGNS, are multi-pass
	algorithms and thus cannot perform incremental model update. To address this
	problem, we present a simple incremental extension of SGNS and provide a
	thorough theoretical analysis to demonstrate its validity. Empirical
	experiments demonstrated the correctness of the theoretical analysis as well as
	the practical usefulness of the incremental algorithm.},
  url       = {TOBEFILLED-e.g, http://www.aclweb.org/anthology/P17-1 37, http://www.aclweb.org/anthology/W17-20 0}
}

