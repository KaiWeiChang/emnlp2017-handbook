SubmissionNumber#=%=#15
FinalPaperTitle#=%=#Learning to Score System Summaries for Better Content Selection Evaluation.
ShortPaperTitle#=%=#Learning to Score System Summaries for Better Content Selection Evaluation.
NumberOfPages#=%=#11
CopyrightSigned#=%=#Maxime Peyrard
JobTitle#==#
Organization#==#Technische Universit채t Darmstadt
Abstract#==#The evaluation of summaries is a challenging but crucial task of the
summarization field. In this work, we propose to learn an automatic scoring
metric based on the human judgements available as part of classical
summarization datasets like TAC-2008 and TAC-2009. Any existing automatic
scoring metrics can be included as features, the model learns the combination
exhibiting the best correlation with human judgments. The reliability of the
new metric is tested in a further manual evaluation where we ask humans to
evaluate summaries covering the whole scoring spectrum of the metric. We
release the trained metric as an open-source tool.
Author{1}{Firstname}#=%=#Maxime
Author{1}{Lastname}#=%=#Peyrard
Author{1}{Email}#=%=#peyrard@aiphes.tu-darmstadt.de
Author{1}{Affiliation}#=%=#Technische Universit채t Darmstadt
Author{2}{Firstname}#=%=#Teresa
Author{2}{Lastname}#=%=#Botschen
Author{2}{Email}#=%=#teresa.botschen@gmail.com
Author{2}{Affiliation}#=%=#UKP Lab, Technische Universit채t Darmstadt
Author{3}{Firstname}#=%=#Iryna
Author{3}{Lastname}#=%=#Gurevych
Author{3}{Email}#=%=#gurevych@ukp.informatik.tu-darmstadt.de
Author{3}{Affiliation}#=%=#UKP Lab, Technische Universit채t Darmstadt

==========