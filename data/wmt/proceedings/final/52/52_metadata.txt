SubmissionNumber#=%=#52
FinalPaperTitle#=%=#Effective Domain Mixing for Neural Machine Translation
ShortPaperTitle#=%=#Effective Domain Mixing for Neural Machine Translation
NumberOfPages#=%=#9
CopyrightSigned#=%=#Reid Pryzant
JobTitle#==#PhD Student
Organization#==#Stanford University 
450 Serra Mall, Stanford, CA 94305
Abstract#==#Neural Machine Translation (NMT) models are often trained on heterogeneous
mixtures of domains, from news to parliamentary proceedings, each with unique
distributions and language. In this work we show that training NMT systems on
naively mixed data can degrade performance versus models fit to each
constituent domain. We demonstrate that this problem can be circumvented, and
propose three models that do so by jointly learning domain discrimination and
translation. We demonstrate the efficacy of these techniques by merging pairs
of domains in three languages: Chinese, French, and Japanese. After training on
composite data, each approach outperforms its domain-specific counterparts,
with a model based on a discriminator network doing so most reliably. We obtain
consistent performance improvements and an average increase of 1.1 BLEU.
Author{1}{Firstname}#=%=#Denny
Author{1}{Lastname}#=%=#Britz
Author{1}{Email}#=%=#dennybritz@gmail.com
Author{1}{Affiliation}#=%=#Google Brain
Author{2}{Firstname}#=%=#Quoc
Author{2}{Lastname}#=%=#Le
Author{2}{Email}#=%=#qvl@google.com
Author{2}{Affiliation}#=%=#Google Brain
Author{3}{Firstname}#=%=#Reid
Author{3}{Lastname}#=%=#Pryzant
Author{3}{Email}#=%=#rpryzant@stanford.edu
Author{3}{Affiliation}#=%=#Stanford University

==========