SubmissionNumber#=%=#73
FinalPaperTitle#=%=#Variable Mini-Batch Sizing and Pre-Trained Embeddings
ShortPaperTitle#=%=#Variable Mini-Batch Sizing and Pre-Trained Embeddings
NumberOfPages#=%=#7
CopyrightSigned#=%=#Mostafa Abdou
JobTitle#==#
Organization#==#ÚFAL MFF UK (Linguistics)
Malostranské náměstí 25
11800 Praha
Czech Republic
Abstract#==#This paper describes our submission to the WMT 2017 Neural MT Training Task.
We modified the provided NMT system in order to allow for interrupting and
continuing the training of models. This allowed mid-training batch size
decrementation and incrementation at variable rates. In addition to the models
with variable batch size, we tried different setups with
pre-trained word2vec embeddings. Aside from batch size incrementation, all our
experiments
performed below the baseline.
Author{1}{Firstname}#=%=#Mostafa
Author{1}{Lastname}#=%=#Abdou
Author{1}{Email}#=%=#mostafahany56@gmail.com
Author{1}{Affiliation}#=%=#UFAL, Charles University
Author{2}{Firstname}#=%=#Vladan
Author{2}{Lastname}#=%=#Gloncak
Author{2}{Email}#=%=#Gloncak@ufal.mff.cuni.cz
Author{2}{Affiliation}#=%=#UFAL, Charles University
Author{3}{Firstname}#=%=#Ondřej
Author{3}{Lastname}#=%=#Bojar
Author{3}{Email}#=%=#bojar@ufal.mff.cuni.cz
Author{3}{Affiliation}#=%=#Charles University in Prague, Faculty of Mathematics and Physics

==========