\begin{bio}

\textbf{Alexander Sasha Rush} is an assistant professor at Harvard University. His research interest is in ML
methods for NLP with recent focus on deep learning for text generation including applications in machine
translation, data and document summarization, and diagram-to-text generation, as well as the development
of the OpenNMT translation system. His past work focused on structured prediction and combinatorial
optimization for NLP.

\textbf{Yoon Kim} is a PhD student at Harvard University. He is interested in deep learning approaches to natural
language processing, and especially in deep models for text generation. He has done previous work in text
classification, language modeling, knowledge distillation, and generation.

\textbf{Sam Wiseman} is a PhD student at Harvard University. He is interested in the intersection of deep learning,
structured prediction, and natural language processing. He has done previous work in coreference
resolution, structured prediction for NLP, and text generation.

\end{bio}

\begin{tutorial}
  {Deep Latent Variable Models of Natural Language}
  {tutorial-final-004}
  {\daydateyear, \tutorialmorningtime}
  {\TutLocD}

The proposed tutorial will cover deep latent variable models both in the case where exact inference over the latent variables is tractable and when it is not. The former case includes neural extensions of unsupervised tagging and parsing models. Our discussion of the latter case, where inference cannot be performed tractably, will restrict itself to continuous latent variables. In particular, we will discuss recent developments both in neural variational inference (e.g., relating to Variational Auto-encoders) and in implicit density modeling (e.g., relating to Generative Adversarial Networks). We will highlight the challenges of applying these families of methods to NLP problems, and discuss recent successes and best practices.

\end{tutorial}
