- id: 1037
  title: Nonparametric Bayesian Semi-supervised Word Segmentation
  authors: Fujii, Ryo and Domoto, Ryo and Mochihashi, Daichi
  abstract: >
    This paper presents a novel hybrid generative/discriminative model of word segmentation based on
    nonparametric Bayesian methods. Unlike ordinary discriminative word segmentation which relies
    only on labeled data, our semi-supervised model also leverages a huge amount of unlabeled text
    to automatically learn new "words'', and further constrains them by using a labeled data to
    segment non-standard texts such as those found in social networking services. Specifically, our
    hybrid model combines a discriminative classifier (CRF; Lafferty et al. (2001)) and unsupervised
    word segmentation (NPYLM; Mochihashi et al. (2009)) with a transparent exchange of information
    between these two model structures within the semi-supervised framework (JESS-CM; Suzuki et
    al. (2008)). We confirmed that it can appropriately segment non-standard texts like those in
    Twitter and Weibo and has nearly state-of-the-art accuracy on standard datasets in Japanese,
    Chinese, and Thai.

- id: 1040
  contact: amansour@cs.sfu.ca
  title: Joint Prediction of Word Alignment with Alignment Types
  authors: Bigvand, Anahita Mansouri and Bu, Te and Sarkar, Anoop
  abstract: >
    Current word alignment models do not distinguish between different types of alignment links. In
    this paper, we provide a new probabilistic model for word alignment where word alignments are
    associated with linguistically motivated alignment types. We propose a novel task of joint
    prediction of word alignment and alignment types and propose novel semi-supervised learning
    algorithms for this task. We also solve a sub-task of predicting the alignment type given an
    aligned word pair. In our experimental results, the generative models we introduce to model
    alignment types significantly outperform the models without alignment types.
    
- id: 1061
  contact: jooyeon.kim@kaist.ac.kr
  title: Joint Modeling of Topics, Citations, and Topical Authority in Academic Corpora
  authors: Kim, Jooyeon and Kim, Dongwoo and Oh, Alice
  abstract: >
    Much of scientific progress stems from previously published findings, but searching through the
    vast sea of scientific publications is difficult. We often rely on metrics of scholarly
    authority to find the prominent authors but these authority indices do not differentiate
    authority based on research topics. We present Latent Topical-Authority Indexing (LTAI) for
    jointly modeling the topics, citations, and topical authority in a corpus of academic
    papers. Compared to previous models, LTAI differs in two main aspects. First, it explicitly
    models the generative process of the citations, rather than treating the citations as
    given. Second, it models each author's influence on citations of a paper based on the topics of
    the cited papers, as well as the citing papers. We fit LTAI to four academic corpora: CORA,
    Arxiv Physics, PNAS, and Citeseer. We compare the performance of LTAI against various baselines,
    starting with the latent Dirichlet allocation, to the more advanced models including author-link
    topic model and dynamic author citation topic model. The results show that LTAI achieves
    improved accuracy over other similar models when predicting words, citations and authors of
    publications.

- id: 1081
  contact: melvinp@google.com
  title: "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"
  authors: Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Viégas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey
  abstract: >
    We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate
    between multiple languages. Our solution requires no changes to the model architecture from a
    standard NMT system but instead introduces an artificial token at the beginning of the input
    sentence to specify the required target language. Using a shared wordpiece vocabulary, our
    approach enables Multilingual NMT using a single model. On the WMT’14 benchmarks, a single
    multilingual model achieves comparable performance for English→French and surpasses
    state-of-the-art results for English→German. Similarly, a single multilingual model surpasses
    state-of-the-art results for French→English and German→English on WMT’14 and WMT’15 benchmarks,
    respectively. On production corpora, multilingual models of up to twelve language pairs allow
    for better translation of many individual pairs. Our models can also learn to perform implicit
    bridging between language pairs never seen explicitly during training, showing that transfer
    learning and zero-shot translation is possible for neural translation. Finally, we show analyses
    that hints at a universal interlingua representation in our models and show some interesting
    examples when mixing languages.

- id: 1083
  contact: vzayats@uw.edu
  title: Conversation Modeling on Reddit using a Graph-Structured LSTM
  authors: Zayats, Vicky and Ostendorf, Mari
  abstract: >
    This paper presents a novel approach for modeling threaded discussions on social media using a
    graph-structured bidirectional LSTM which represents both hierarchical and temporal conversation
    structure. In experiments with a task of predicting popularity of comments in Reddit
    discussions, the proposed model outperforms a node-independent architecture for different sets
    of input features. Analyses show a benefit to the model over the full course of the discussion,
    improving detection in both early and late stages. Further, the use of language cues with the
    bidirectional tree state updates helps with identifying controversial comments.

- id: 1142
  contact: wmonroe4@stanford.edu
  title: "Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding"
  authors: Monroe, Will and Hawkins, Robert X.D. and Goodman, Noah D. and Potts, Christopher
  abstract: >
    We present a model of pragmatic referring expression interpretation in a grounded communication
    task (identifying colors from descriptions) that draws upon predictions from two recurrent
    neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning
    framework. Experiments show that this combined pragmatic model interprets color descriptions
    more accurately than the classifiers from which it is built, and that much of this improvement
    results from combining the speaker and listener perspectives. We observe that pragmatic
    reasoning helps primarily in the hardest cases: when the model must distinguish very similar
    colors, or when few utterances adequately express the target color. Our findings make use of a
    newly-collected corpus of human utterances in color reference games, which exhibit a variety of
    pragmatic behaviors. We also show that the embedded speaker model reproduces many of these
    pragmatic behaviors.

- id: 1170
  contact: jkk@cs.berkeley.edu
  title: "Parsing with Traces: An O(n^4) Algorithm and a Structural Representation"
  authors: Kummerfeld, Jonathan K. and Klein, Dan
  abstract: >
    General treebank analyses are graph structured, but parsers are typically restricted to tree
    structures for efficiency and modeling reasons. We propose a new representation and algorithm
    for a class of graph structures that is flexible enough to cover almost all treebank structures,
    but still admit efficient learning and inference. In particular, we consider directed, acyclic,
    one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared
    argumentation, and similar tree-violating linguistic phenomena. We describe how to convert
    phrase structure parses, including traces, to our new representation, in a reversible
    manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers
    97.3% of the Penn English treebank. We also implement a proof-of-concept parser that recovers a
    range of null elements and trace types.

- id: 1171
  contact: nikola.mrksic@gmail.com
  title: Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints
  authors: Mrkšić, Nikola and Vulić, Ivan and Séaghdha, Diarmuid Ó and Leviant, Ira and Reichart, Roi and Gašić, Milica and Korhonen, Anna and Young, Steve
  abstract: >
    We present Attract-Repel, an algorithm for improving the semantic quality of word vectors by
    injecting constraints extracted from lexical resources. Attract-Repel facilitates the use of
    constraints from mono- and cross-lingual resources, yielding semantically specialised
    cross-lingual vector spaces. Our evaluation shows that the method can make use of existing
    cross-lingual lexicons to construct high-quality vector spaces for a plethora of different
    languages, facilitating semantic transfer from high- to lower-resource ones. The effectiveness
    of our approach is demonstrated with state-of-the-art results on semantic similarity datasets in
    six languages. We next show that Attract-Repel-specialised vectors boost performance in the
    downstream task of dialogue state tracking (DST) across multiple languages. Finally, we show
    that cross-lingual vector spaces produced by our algorithm facilitate the training of
    multilingual DST models, which brings further performance improvements.

- id: 1055
  contact: nhf@umd.edu
  title: Evaluating Low-Level Speech Features Against Human Perceptual Data
  authors: Richter, Caitlin and Feldman, Naomi H. and Salgado, Harini and Jansen, Aren
  abstract: >
    We introduce a method for measuring the correspondence between low-level speech features and
    human perception, using a cognitive model of speech perception implemented directly on speech
    recordings.  We evaluate two speaker normalization techniques using this method and find that in
    both cases, speech features that are normalized across speakers predict human data better than
    unnormalized speech features, consistent with previous research.  Results further reveal
    differences across normalization methods in how well each predicts human data.  This work
    provides a new framework for evaluating low-level representations of speech on their match to
    human perception, and lays the groundwork for creating more ecologically valid models of speech
    perception.
