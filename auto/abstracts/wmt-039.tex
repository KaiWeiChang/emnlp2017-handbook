The WMT17 Neural Machine Translation Training Task aimed to test various methods of training neural machine translation systems. We describe the AFRL submission, including preprocessing and its knowledge distillation framework. The teacher systems are given factors for domain, case, and subword location. The student systems are given multiple teachers' output and a subselected set of the training data designed to match the target domain. Numerical results indicate that the student systems surpass the teachers in translation quality and that this benefit comes directly from the inclusion of the teachers' output.
