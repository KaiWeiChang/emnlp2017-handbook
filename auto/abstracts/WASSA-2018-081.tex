We present BrainT, a multiclass, averaged perceptron tested on implicit emotion prediction of tweets. We show that the dataset is linearly separable and explore ways in fine-tuning the baseline classifier. Our results indicate that the bag-of-words features benefit the model moderately and prediction can be improved significantly with bigrams, trigrams, skip-one- tetragrams and POS-tags.  Furthermore, we find preprocessing of the n-grams, including stemming, lowercasing, stopword filtering, emoji and emoticon conversion, generally not useful. The model is trained on an annotated corpus of 153,383 tweets and predictions on the test data were submitted to the  WASSA-2018 Implicit Emotion Shared Task. BrainT attained a Macro F-score of 0.63.
