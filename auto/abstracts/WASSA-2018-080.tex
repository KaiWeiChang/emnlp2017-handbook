We describe UBC-NLP contribution to IEST-2018,  focused  at  learning  implicit  emotion in  Twitter  data.   Among  the 30 participating teams, our system ranked the 4th (with 69.3\%F-score).   Post competition,  we were able to score slightly higher than the 3rd ranking system (reaching 70.7\%).  Our system is trained on top of a pre-trained language model (LM),fine-tuned on the data provided by the task organizers.  Our best results are acquired by an average  of  an  ensemble  of  language  models.We  also  offer  an  analysis  of  system  performance and the impact of training data size on the task.  For example, we show that training our best model for only one epoch with<40\%of the data enables better performance than the baseline reported by Klinger et al. (2018) for the task.
