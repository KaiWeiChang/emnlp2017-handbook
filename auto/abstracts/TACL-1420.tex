Can advances in NLP help guide linguistic theory? We examine the role of neural networks, the current state of the art in many common NLP tasks. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter, Pinker and Prince (1988) presented a comprehensive rebuttal of many of Rumelhart and McClelland\'92s claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland (1986) model. Today, however, that model is severely outmoded. We show that the recurrent neural networks in modern NLP systems obviate most of Pinker and Prince\'92s criticisms. We suggest that the empirical performance of recurrent neural networks warrants a reexamination of their utility in linguistic and cognitive modeling.
