Named Entity Disambiguation algorithms typically learn a single model for all target entities.  In this paper we present a word expert model and train separate deep learning models for each target entity string, yielding 500K classification tasks. This gives us the  opportunity to benchmark popular text representation alternatives on this massive dataset. In order to face scarce training data we propose a simple data-augmentation technique and transfer-learning.  We show that bag-of-word-embeddings are better than LSTMs for tasks with scarce training data, while the situation is reversed when having larger amounts. Transferring a LSTM which is learned on all datasets is the most effective context representation option for the word experts in all frequency bands. The experiments show that our system trained  on out-of-domain Wikipedia data surpass comparable NED systems which have been trained on in-domain training data.
