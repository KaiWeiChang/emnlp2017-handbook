This work describes the AMU-UEdin submission to the WMT 2017 shared task on Automatic Post-Editing. We explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output.  We focus on neural end-to-end models that combine both inputs \$\mt\$ and \$\src\$ in a single neural architecture, modeling \$\{\mt, \src\} \rightarrow \pe\$ directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas.
