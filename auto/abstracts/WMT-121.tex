This paper describes the University of Maryland's submission to the WMT 2018 Chinese-English news translation tasks. Our systems are BPE-based self-attentional Transformer networks with parallel and backtranslated monolingual training data. Using ensembling and reranking, we improve over the transformer baseline by 1.4 BLEU for Chinese-English and 3.97 BLEU for English-Chinese on newstest2017. Our best systems reach BLEU scores of 24.4 for Chinese-English and 39.0 for English-Chinese on newstest2018.
