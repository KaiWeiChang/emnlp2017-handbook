Spontaneous spoken dialogue is often disfluent, containing pauses, hesitations, self-corrections and false starts. Processing such phenomena is essential in understanding a speaker's intended meaning and controlling the flow of the conversation. Furthermore, this processing needs to be word-by-word incremental. From a developer's point of view, it is highly desirable to be able to develop systems which can be trained from `clean' examples while able to generalise to the very diverse disfluent variations on the same data -- thereby enhancing both data-efficiency and robustness. In this paper, we present a multi-task LSTM-based model for disfluency detection, which is able to predict disfluency structure incrementally, word-by-word. We train the system on the Switchboard Dialogue Acts (SWDA) corpus and present its accuracy on this dataset. Our model outperforms prior neural network-based incremental approaches by about 10 percentage points on SWDA while employing a simpler architecture. To test generalisation potential, we evaluate the same model on bAbI+, without any more training. bAbI+ is a dataset of synthesised goal-oriented dialogues where we control the distribution of disfluencies \& their types. This shows that our approach has good generalisation potential, and sheds more light on what types of disfluency might be less amenable to domain-general processing.
