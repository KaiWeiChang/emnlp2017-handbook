We created a NMT model with character level encoding. Scheduled preprocessing like Tokenization, Truecasing and Cleaning were done initially. Since, we designed a Sequence to Sequence model, it had two parts, viz., an Encoder and a Decoder. We used one LSTM layers each for the encoder and decoder. The batch size was set to 128, number of epochs was set to 100, activation function was softmax, optimizer chosen was rmsprop and loss function used was categorical cross-entropy. Learning rate was set to 0.001.
