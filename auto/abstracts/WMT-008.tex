Our entry to the parallel corpus filtering task uses a two step strategy.  The first step uses a series of pragmatic hard â€˜rules' to remove the worst example sentences.  This first step reduces the effective corpus size down from the initial 1 billion to 160 million tokens. The second step uses four different heuristics weighted to produce a score that is then used for further filtering down to 100 or 10 million tokens.  Our final system produces competitive results without requiring excessive fine tuning to the exact task or language pair. The first step in isolation provides a very fast filter that gives most of the gains of the final system.
