Sequence-to-Sequence (seq2seq) models have become overwhelmingly popular in build- ing end-to-end trainable dialogue systems. Though highly efficient in learning the back- bone of human-computer communications, they suffer from the problem of strongly fa- voring short generic responses. In this pa- per, we argue that a good response should smoothly connect both the preceding dialogue history and the following conversations. We strengthen this connection by mutual infor- mation maximization. To sidestep the non- differentiability of discrete natural language tokens, we introduce an auxiliary continuous code space and map such code space to a learn- able prior distribution for generation purpose. Experiments on two dialogue datasets validate the effectiveness of our model, where the gen- erated responses are closely related to the dia- logue context and lead to more interactive con- versations.
