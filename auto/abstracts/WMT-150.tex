The paper presents two approaches to word level QE - (i) Bag of Words (BoW) model, and (ii) Paragraph Vector (Doc2Vec) model. In the BoW model, bag of words are prepared from source sentences for each target word appearing in both MT and PE output in the training data. For every target word appearing in the MT output in the development set, the cosine similarity  between (a) the corresponding source sentence and (b) the bag of words for the same target word has been computed. From this result the threshold (for the target word) has been derived above which the word is retained(i.e. `OK'). In the Doc2Vec based approach to word level QE, for each target word appearing in both MT and PE output in the training data, two document vectors are prepared from (i) the corresponding source sentences and (ii) from the bag of words (as in the BoW model) of the target word. Next,  the similarity between these two document vectors for every target word is being figured out. From these Doc2Vec similarity score and the corresponding PE decision (i.e., whether the target word is retained in PE, or not, in the training dataset),  a system level threshold has been computed. For the testset sentences, if the Doc2Vec similarity score for a target word exceeds this threshold value, then the target word is retained(i.e. `OK'), otherwise it is dropped(i.e. `BAD').
