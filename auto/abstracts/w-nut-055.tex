Unsupervised word translation is a promising research area to induce bilingual dictionaries without using any human-generated dictionaries. Recent state-of-the-art approaches typically learn a linear transformation matrix \$W\$ to map pre-trained word embeddings from one language to another, which enables to learn a shared word embedding space across two different languages. The simple linear transformation has proven to be effective by using orthogonality constraints. However, we found that accuracy of the linear method dramatically drops on unsupervised English-to-English word translation in different domains, e.g. Wikipedia articles and Amazon reviews. To better capture such distribution shifts across domains, we propose two modifications: (1) using a nonlinear transformation function and (2) adding an inverse transformation function to map embeddings in the target space back into the source space motivated by CycleGAN. Our preliminary experiments show that the additional adversarial learning with the inverse function improves the model training, and the nonlinearity with CycleGAN has the potential to improve the accuracy of the different domain setting.
