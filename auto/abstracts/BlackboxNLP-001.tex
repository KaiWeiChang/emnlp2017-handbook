This work aims to contribute to our understanding of {\em when}{\textasciitilde}multi-task learning through parameter sharing in deep neural networks leads to improvements over single-task learning. We focus on the setting of learning from {\em loosely related}{\textasciitilde}tasks, for which no theoretical guarantees exist. We therefore approach the question empirically, studying which properties of datasets and single-task learning characteristics correlate with improvements from multi-task learning. We are the first to study this in a text classification setting and across more than 500 different task pairs.
