We introduce a novel framework for delexicalized dependency parsing in a new language. We show that useful features of the language can be extracted automatically from an un- parsed corpus, which consists only of gold part-of-speech (POS) sequences. Providing these features to our neural parser enables it to parse sequences of this kind. Strikingly, our system has no supervision in the target language. Rather, it is a multilingual system that is trained end-to-end on a variety of other languages, so it learns a feature extractor that works well. We show experimentally across multiple languages: (1) Features computed from the unparsed corpus improve parsing accuracy. (2) Including thousands of synthetic languages in the training achieves further improvement. (3) Despite being computed from unparsed corpora, our learned task-specific features beat previous work\'92s interpretable typological features that require parsed corpora or expert categorization. Our best method improved attachment scores on held-out test languages by an average of 5.6 percentage points over past work that does not inspect the unparsed data (McDonald et al., 2011), and by 18.8 points over past \'93grammar induction\'94 work that does not use training languages (Naseem et al., 2010).
